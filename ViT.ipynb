{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17472aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b78f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c424ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9144 files belonging to 102 classes.\n",
      "Using 7316 files for training.\n",
      "Found 9144 files belonging to 102 classes.\n",
      "Using 1828 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# load images to train a model\n",
    "# For shorter training time, We'll use caltech101 instead of imagenet used in the paper\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(r'C:\\Users\\K\\tensorflow_datasets\\caltech101')\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                       label_mode='categorical',\n",
    "                                                       validation_split=0.2,\n",
    "                                                       subset=\"training\",\n",
    "                                                       seed=123,\n",
    "                                                       image_size=(img_height, img_width),\n",
    "                                                       batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                     label_mode='categorical',\n",
    "                                                     validation_split=0.2,\n",
    "                                                     subset=\"validation\",\n",
    "                                                     seed=123,\n",
    "                                                     image_size=(img_height, img_width),\n",
    "                                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143729d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch(tf.keras.layers.Layer):\n",
    "    \"\"\"coverts input images to patches\"\"\"\n",
    "    def __init__(self, patch_size, **kwards):\n",
    "        super(Patch, self).__init__(**kwards)\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        patches = self.convert_to_patches(inputs, self.patch_size)\n",
    "        return patches\n",
    "    \n",
    "    def convert_to_patches(self, images, patch_size):\n",
    "        \"\"\"convert batch of images to batch of flattened patches\"\"\"\n",
    "        # shape of images : (batch_size, width, height, channels)\n",
    "        # shape of output : (batch_size, no. of flattened patches, patch_size)\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images, \n",
    "                                           sizes=[1, patch_size, patch_size, 1], \n",
    "                                           strides=[1, patch_size, patch_size, 1], \n",
    "                                           rates=[1, 1, 1, 1], \n",
    "                                           padding='VALID')\n",
    "        flattened_size = tf.shape(patches)[-1]\n",
    "        patches = tf.reshape(patches, shape=[batch_size, -1, flattened_size])\n",
    "    \n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99e60a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 256, 3)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 64, 3072)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(type(images))\n",
    "    patches = Patch(32)(images)\n",
    "    print(patches.shape)\n",
    "    print(type(patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6462eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(tf.keras.layers.Layer):\n",
    "    \"\"\"linear projection of flattened patches\"\"\"\n",
    "    def __init__(self, d_model, **kwards):\n",
    "        super(Projection, self).__init__(**kwards)\n",
    "        self.d_model = d_model\n",
    "        self.another = tf.keras.layers.Dense(units=d_model)\n",
    "        self.project = tf.keras.layers.Dense(units=d_model)\n",
    "        self.cls_token = self.add_weight(name='class token',\n",
    "                                        shape=(1, 1, d_model),\n",
    "                                        initializer=tf.initializers.RandomNormal(),\n",
    "                                        trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        cls_token = tf.tile(self.cls_token, [tf.shape(inputs)[0], 1, 1])\n",
    "        inputs = self.another(inputs)\n",
    "        inputs = self.project(inputs)\n",
    "        return tf.concat([inputs, cls_token], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8545505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65, 50)\n",
      "(32, 66, 50)\n"
     ]
    }
   ],
   "source": [
    "print(patches.shape)\n",
    "patches = Projection(50)(patches)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e728fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pos_embedding(tf.keras.layers.Layer):\n",
    "    \"\"\"add standard 1D positional embedding\"\"\"\n",
    "    def __init__(self, **kwards):\n",
    "        super(Pos_embedding, self).__init__(**kwards)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.pos_embedding = self.add_weight(name='pos_embedding',\n",
    "                                            shape=(1, input_shape[1], input_shape[2]),\n",
    "                                            initializer=tf.initializers.RandomNormal(),\n",
    "                                            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dc6270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 66, 50)\n",
      "(32, 66, 50)\n"
     ]
    }
   ],
   "source": [
    "print(patches.shape)\n",
    "patches = Pos_embedding()(patches)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85205d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    \"\"\"MLP layer in encoder of the transformer\"\"\"\n",
    "    def __init__(self, d_model, mlp_dim, dropout_rate, **kwards):\n",
    "        super(MLP, self).__init__(**kwards)\n",
    "        self.net = tf.keras.Sequential([tf.keras.layers.Dense(mlp_dim, activation='relu'),\n",
    "                                      tf.keras.layers.Dropout(dropout_rate),\n",
    "                                      tf.keras.layers.Dense(d_model),\n",
    "                                      tf.keras.layers.Dropout(dropout_rate)])\n",
    "    def call(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b05f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable\n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e101edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0781a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "    def __init__(self, d_model,\n",
    "                 mlp_dim, num_heads, dropout_rate, use_bias=False,\n",
    "                 **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.mlp = MLP(d_model=d_model, mlp_dim=mlp_dim, dropout_rate=dropout_rate)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, X, mask=False):\n",
    "        output = self.layernorm1(X)\n",
    "        output = self.dropout1(output)\n",
    "        output = self.attention(v=X, k=X, q=X, mask=None) + X\n",
    "        \n",
    "        output2 = self.layernorm2(output)\n",
    "        output2 = self.dropout2(output2)\n",
    "        output2 = self.mlp(output2) + output\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c629b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 66, 50)\n",
      "(32, 66, 50)\n"
     ]
    }
   ],
   "source": [
    "print(patches.shape)\n",
    "patches = EncoderBlock(50, 100, 10, 0.1)(patches, None)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ae77611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(tf.keras.Model):\n",
    "    \"\"\"Vision Transformer model\"\"\"\n",
    "    def __init__(self, d_model, mlp_dim,\n",
    "                 num_heads, dropout_rate, num_layers, \n",
    "                 patch_size, num_classes, use_bias=False, **kwards):\n",
    "        super(ViT, self).__init__(**kwards)\n",
    "        self.d_model = d_model\n",
    "        self.patch = Patch(patch_size)\n",
    "        self.projection = Projection(d_model)\n",
    "        self.pos_embedding = Pos_embedding()\n",
    "        self.blocks = []\n",
    "        for _ in range(num_layers):\n",
    "            self.blocks.append(EncoderBlock(d_model, mlp_dim, num_heads, dropout_rate, use_bias))\n",
    "        self.mlp_head = tf.keras.Sequential([tf.keras.layers.LayerNormalization(epsilon=1e-6),\n",
    "                                            tf.keras.layers.Dense(num_classes)])\n",
    "    \n",
    "    def call(self, X):\n",
    "        X = self.patch(X)\n",
    "        X = self.projection(X)\n",
    "        X = self.pos_embedding(X)\n",
    "        for blk in self.blocks:\n",
    "            X = blk(X)\n",
    "        X = X[:, 0]\n",
    "        X = self.mlp_head(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "693bf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 256, 3)\n",
      "(32, 102)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "    result = ViT(50, 100, 10, 0.1, 3, 32, 102)(images)\n",
    "    print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9d42d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vi_t_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "patch_27 (Patch)             multiple                  0         \n",
      "_________________________________________________________________\n",
      "projection_28 (Projection)   multiple                  156250    \n",
      "_________________________________________________________________\n",
      "pos_embedding_22 (Pos_embedd multiple                  3250      \n",
      "_________________________________________________________________\n",
      "encoder_block_57 (EncoderBlo multiple                  20550     \n",
      "_________________________________________________________________\n",
      "encoder_block_58 (EncoderBlo multiple                  20550     \n",
      "_________________________________________________________________\n",
      "encoder_block_59 (EncoderBlo multiple                  20550     \n",
      "_________________________________________________________________\n",
      "sequential_85 (Sequential)   (32, 102)                 5302      \n",
      "=================================================================\n",
      "Total params: 226,452\n",
      "Trainable params: 226,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ViT(50, 100, 10, 0.001, 3, 32, 102)\n",
    "model.build((32,256,256,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = ViT(50, 100, 10, 0.001, 3, 32, 102)\n",
    "\n",
    "def loss(model, x, y, training):\n",
    "    # training=training is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_ = model(x, training=training)\n",
    "\n",
    "    return tf.losses.CategoricalCrossentropy(from_logits=True)(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss =tf.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for x, y in train_ds:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_accuracy.update_state(y, model(x, training=True))\n",
    "\n",
    "    # End epoch\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Accuracy: {:.3%}\".format(epoch, epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb721661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
